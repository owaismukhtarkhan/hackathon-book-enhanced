"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[717],{757(e,n,o){o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>c,toc:()=>l});var i=o(4848),t=o(8453);const s={title:"Speech Recognition for Robotics",sidebar_position:2},r="Speech Recognition for Robotics",c={id:"modules/week-13-conversational/speech-recognition",title:"Speech Recognition for Robotics",description:"Overview",source:"@site/docs/modules/week-13-conversational/speech-recognition.md",sourceDirName:"modules/week-13-conversational",slug:"/modules/week-13-conversational/speech-recognition",permalink:"/hackathon-book-enhanced/docs/modules/week-13-conversational/speech-recognition",draft:!1,unlisted:!1,editUrl:"https://github.com/owaismukhtarkhan/hackathon-book/tree/main/docs/modules/week-13-conversational/speech-recognition.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Speech Recognition for Robotics",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Language Models in Robotics",permalink:"/hackathon-book-enhanced/docs/modules/week-13-conversational/language-models-robotics"},next:{title:"Multi-Modal Interaction in Robotics",permalink:"/hackathon-book-enhanced/docs/modules/week-13-conversational/multi-modal-interaction"}},a={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Speech Recognition in Robotics Context",id:"speech-recognition-in-robotics-context",level:2},{value:"Why Speech Recognition for Robots?",id:"why-speech-recognition-for-robots",level:3},{value:"Challenges in Robotic Speech Recognition",id:"challenges-in-robotic-speech-recognition",level:3},{value:"Speech Recognition Approaches",id:"speech-recognition-approaches",level:2},{value:"Cloud-Based Speech Recognition",id:"cloud-based-speech-recognition",level:3},{value:"Google Cloud Speech-to-Text",id:"google-cloud-speech-to-text",level:4},{value:"Local Speech Recognition",id:"local-speech-recognition",level:3},{value:"Using SpeechRecognition Library",id:"using-speechrecognition-library",level:4},{value:"Integration with Robot Systems",id:"integration-with-robot-systems",level:2},{value:"ROS 2 Speech Recognition Node",id:"ros-2-speech-recognition-node",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"Combining Speech Recognition with Language Models",id:"combining-speech-recognition-with-language-models",level:3},{value:"Handling Recognition Errors and Uncertainties",id:"handling-recognition-errors-and-uncertainties",level:2},{value:"Confidence Scoring and Error Handling",id:"confidence-scoring-and-error-handling",level:3},{value:"Practical Exercise: Voice Command Robot Interface",id:"practical-exercise-voice-command-robot-interface",level:2},{value:"Objective",id:"objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Complete System Architecture",id:"complete-system-architecture",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"speech-recognition-for-robotics",children:"Speech Recognition for Robotics"}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This module covers speech recognition systems for robotics applications, focusing on converting spoken language to text that can be processed by language models like Google Gemini. We'll explore both local and cloud-based speech recognition solutions and their integration with robotic systems."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Set up speech recognition systems for robotic applications"}),"\n",(0,i.jsx)(n.li,{children:"Integrate speech recognition with language models"}),"\n",(0,i.jsx)(n.li,{children:"Handle speech recognition errors and uncertainties"}),"\n",(0,i.jsx)(n.li,{children:"Implement robust voice command processing for robots"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completed language models in robotics module"}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of audio processing"}),"\n",(0,i.jsx)(n.li,{children:"Completed ROS 2 fundamentals"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"speech-recognition-in-robotics-context",children:"Speech Recognition in Robotics Context"}),"\n",(0,i.jsx)(n.h3,{id:"why-speech-recognition-for-robots",children:"Why Speech Recognition for Robots?"}),"\n",(0,i.jsx)(n.p,{children:"Speech recognition enables natural human-robot interaction:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hands-free operation"}),": Users can control robots without physical interfaces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural communication"}),": More intuitive than button-based interfaces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accessibility"}),": Enables interaction for users with mobility limitations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multimodal interaction"}),": Combines with vision and other sensors"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"challenges-in-robotic-speech-recognition",children:"Challenges in Robotic Speech Recognition"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noisy environments"}),": Robots operate in various acoustic conditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time processing"}),": Need for low-latency response"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vocabulary constraints"}),": Limited domain-specific commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Must handle various accents and speaking styles"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"speech-recognition-approaches",children:"Speech Recognition Approaches"}),"\n",(0,i.jsx)(n.h3,{id:"cloud-based-speech-recognition",children:"Cloud-Based Speech Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Cloud-based solutions offer high accuracy but require network connectivity:"}),"\n",(0,i.jsx)(n.h4,{id:"google-cloud-speech-to-text",children:"Google Cloud Speech-to-Text"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Install required package\npip install google-cloud-speech\n\n# Example implementation\nfrom google.cloud import speech\nimport pyaudio\nimport io\n\ndef transcribe_audio_cloud(audio_data):\n    """Transcribe audio using Google Cloud Speech-to-Text"""\n    client = speech.SpeechClient()\n\n    audio = speech.RecognitionAudio(content=audio_data)\n    config = speech.RecognitionConfig(\n        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n        sample_rate_hertz=16000,\n        language_code="en-US",\n    )\n\n    response = client.recognize(config=config, audio=audio)\n\n    for result in response.results:\n        return result.alternatives[0].transcript\n\n    return ""\n'})}),"\n",(0,i.jsx)(n.h3,{id:"local-speech-recognition",children:"Local Speech Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Local solutions work without network connectivity but may have lower accuracy:"}),"\n",(0,i.jsx)(n.h4,{id:"using-speechrecognition-library",children:"Using SpeechRecognition Library"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\n\nclass LocalSpeechRecognizer:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n    def listen_for_command(self):\n        """Listen for and transcribe a voice command"""\n        with self.microphone as source:\n            print("Listening for command...")\n            audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=5)\n\n        try:\n            # Use Google Web Speech API (free, limited)\n            command = self.recognizer.recognize_google(audio)\n            print(f"Recognized: {command}")\n            return command\n        except sr.UnknownValueError:\n            print("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            print(f"Error with speech recognition service: {e}")\n            return None\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-robot-systems",children:"Integration with Robot Systems"}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-speech-recognition-node",children:"ROS 2 Speech Recognition Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# speech_recognition_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport speech_recognition as sr\nimport threading\nimport queue\n\nclass SpeechRecognitionNode(Node):\n    def __init__(self):\n        super().__init__(\'speech_recognition_node\')\n\n        # Publisher for recognized text\n        self.text_publisher = self.create_publisher(\n            String,\n            \'recognized_speech\',\n            10\n        )\n\n        # Publisher for voice activity\n        self.vad_publisher = self.create_publisher(\n            String,\n            \'voice_activity\',\n            10\n        )\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Set energy threshold for voice activity detection\n        self.recognizer.energy_threshold = 4000  # Adjust based on environment\n\n        # Create a queue to share audio data between threads\n        self.audio_queue = queue.Queue()\n\n        # Start speech recognition in a separate thread\n        self.recognition_thread = threading.Thread(target=self.recognition_worker)\n        self.recognition_thread.daemon = True\n        self.recognition_thread.start()\n\n        self.get_logger().info(\'Speech recognition node initialized\')\n\n    def recognition_worker(self):\n        """Worker function for speech recognition in separate thread"""\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source, duration=1)\n\n        # Use a callback for continuous listening\n        stop_listening = self.recognizer.listen_in_background(\n            self.microphone,\n            self.audio_callback\n        )\n\n        # Keep the thread alive\n        try:\n            while rclpy.ok():\n                # Process any queued audio\n                try:\n                    audio = self.audio_queue.get(timeout=1)\n                    self.process_audio(audio)\n                except queue.Empty:\n                    continue\n        except KeyboardInterrupt:\n            stop_listening(wait_for_stop=False)\n\n    def audio_callback(self, recognizer, audio):\n        """Callback for when audio is captured"""\n        # Put audio in queue for processing\n        self.audio_queue.put(audio)\n\n        # Publish voice activity detection\n        vad_msg = String()\n        vad_msg.data = "speech_detected"\n        self.vad_publisher.publish(vad_msg)\n\n    def process_audio(self, audio):\n        """Process audio and publish recognized text"""\n        try:\n            # Recognize speech using Google Web Speech API\n            text = recognizer.recognize_google(audio)\n\n            # Publish recognized text\n            text_msg = String()\n            text_msg.data = text\n            self.text_publisher.publish(text_msg)\n\n            self.get_logger().info(f\'Recognized: {text}\')\n\n        except sr.UnknownValueError:\n            self.get_logger().info(\'Speech not understood\')\n        except sr.RequestError as e:\n            self.get_logger().error(f\'Speech recognition error: {e}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    speech_node = SpeechRecognitionNode()\n\n    try:\n        rclpy.spin(speech_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        speech_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"combining-speech-recognition-with-language-models",children:"Combining Speech Recognition with Language Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# voice_command_processor.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport google.generativeai as genai\nimport json\nimport speech_recognition as sr\n\nclass VoiceCommandProcessor(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_processor\')\n\n        # Subscribe to recognized speech\n        self.speech_subscription = self.create_subscription(\n            String,\n            \'recognized_speech\',\n            self.speech_callback,\n            10\n        )\n\n        # Publisher for processed commands\n        self.command_publisher = self.create_publisher(\n            String,\n            \'processed_commands\',\n            10\n        )\n\n        # Publisher for robot actions\n        self.action_publisher = self.create_publisher(\n            String,\n            \'robot_actions\',\n            10\n        )\n\n        # Initialize Gemini model\n        self.model = genai.GenerativeModel(\'gemini-pro\')\n\n        # Command confidence threshold\n        self.confidence_threshold = 0.7\n\n        self.get_logger().info(\'Voice command processor initialized\')\n\n    def speech_callback(self, msg):\n        """Process recognized speech command"""\n        try:\n            # Get robot state and context\n            robot_state = self.get_robot_state()\n            environment_context = self.get_environment_context()\n\n            # Process command with Gemini\n            processed_command = self.process_command_with_gemini(\n                msg.data,\n                robot_state,\n                environment_context\n            )\n\n            # Check confidence threshold\n            if processed_command.get(\'confidence_score\', 0) >= self.confidence_threshold:\n                # Publish processed command\n                command_msg = String()\n                command_msg.data = json.dumps(processed_command)\n                self.command_publisher.publish(command_msg)\n\n                # Publish robot actions\n                action_msg = String()\n                action_msg.data = json.dumps(processed_command.get(\'action_sequence\', []))\n                self.action_publisher.publish(action_msg)\n\n                self.get_logger().info(f\'Executed command: {msg.data}\')\n            else:\n                self.get_logger().warn(\n                    f\'Command confidence too low: {processed_command.get("confidence_score", 0)}\'\n                )\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing speech: {e}\')\n\n    def process_command_with_gemini(self, command, robot_state, environment_context):\n        """Process command using Gemini with context"""\n        prompt = f"""\n        You are a robot command interpreter. Interpret the following voice command\n        in the context of the robot\'s current state and environment.\n\n        Robot State: {json.dumps(robot_state)}\n        Environment Context: {json.dumps(environment_context)}\n        Voice Command: {command}\n\n        Provide a JSON response with:\n        1. action_sequence: List of actions to execute\n        2. confidence_score: Confidence in interpretation (0.0 to 1.0)\n        3. reasoning: Brief explanation of interpretation\n        4. parameters: Any needed parameters for actions\n        5. clarification_needed: Boolean if command is ambiguous\n        """\n\n        try:\n            response = self.model.generate_content(prompt)\n            result = json.loads(response.text.strip().strip(\'```json\').strip(\'`\'))\n            return result\n        except (json.JSONDecodeError, Exception) as e:\n            self.get_logger().error(f\'Error processing with Gemini: {e}\')\n            return {\n                "action_sequence": ["unknown"],\n                "confidence_score": 0.0,\n                "reasoning": "Failed to process command",\n                "parameters": {},\n                "clarification_needed": True\n            }\n\n    def get_robot_state(self):\n        """Get current robot state"""\n        return {\n            "position": {"x": 0.0, "y": 0.0, "theta": 0.0},\n            "battery": 0.85,\n            "gripper": "open",\n            "current_task": "idle"\n        }\n\n    def get_environment_context(self):\n        """Get environment context"""\n        return {\n            "objects": ["table", "chair", "cup"],\n            "rooms": ["kitchen", "living_room"],\n            "navigation_goals": ["kitchen", "bedroom"]\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = VoiceCommandProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"handling-recognition-errors-and-uncertainties",children:"Handling Recognition Errors and Uncertainties"}),"\n",(0,i.jsx)(n.h3,{id:"confidence-scoring-and-error-handling",children:"Confidence Scoring and Error Handling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RobustSpeechProcessor:\n    def __init__(self):\n        self.min_confidence = 0.7\n        self.retry_count = 3\n\n    def process_with_confidence(self, audio_data):\n        """Process audio with confidence scoring"""\n        # This is a simplified example\n        # In practice, you\'d use the recognition service\'s confidence scores\n        recognized_text = self.recognize_audio(audio_data)\n\n        # Estimate confidence based on various factors\n        confidence = self.estimate_confidence(recognized_text, audio_data)\n\n        if confidence >= self.min_confidence:\n            return recognized_text, confidence\n        else:\n            # Request clarification or retry\n            return self.handle_low_confidence(recognized_text, confidence)\n\n    def estimate_confidence(self, text, audio_data):\n        """Estimate confidence in recognition result"""\n        # Implement confidence estimation logic\n        # Consider factors like audio quality, word probabilities, etc.\n        return 0.8  # Placeholder\n\n    def handle_low_confidence(self, text, confidence):\n        """Handle cases where confidence is too low"""\n        # Ask for clarification, repeat command, or use fallback\n        return text, confidence\n'})}),"\n",(0,i.jsx)(n.h2,{id:"practical-exercise-voice-command-robot-interface",children:"Practical Exercise: Voice Command Robot Interface"}),"\n",(0,i.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete voice command system that converts speech to robot actions."}),"\n",(0,i.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up speech recognition node"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with language model processing"}),"\n",(0,i.jsx)(n.li,{children:"Implement command execution pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Test with various voice commands"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Microphone \u2192 Audio Capture \u2192 Speech Recognition \u2192 Text \u2192 Language Model \u2192 Robot Actions\n"})}),"\n",(0,i.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,i.jsx)(n.p,{children:"Students must demonstrate the following:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Successfully set up speech recognition for robot commands"}),"\n",(0,i.jsx)(n.li,{children:"Integrate speech recognition with language model processing"}),"\n",(0,i.jsx)(n.li,{children:"Handle recognition errors and uncertainties appropriately"}),"\n",(0,i.jsx)(n.li,{children:"Achieve 80% success rate in voice command processing"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Success Threshold: 80% success rate in voice command processing"})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"After completing this module, proceed to multi-modal interaction to create a complete conversational robotics system that combines vision, language, and speech recognition."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,o){o.d(n,{R:()=>r,x:()=>c});var i=o(6540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);