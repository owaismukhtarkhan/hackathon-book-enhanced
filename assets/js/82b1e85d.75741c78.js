"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[322],{5645(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var t=i(4848),s=i(8453);const o={title:"Multi-Modal Interaction in Robotics",sidebar_position:3},a="Multi-Modal Interaction in Robotics",r={id:"modules/week-13-conversational/multi-modal-interaction",title:"Multi-Modal Interaction in Robotics",description:"Overview",source:"@site/docs/modules/week-13-conversational/multi-modal-interaction.md",sourceDirName:"modules/week-13-conversational",slug:"/modules/week-13-conversational/multi-modal-interaction",permalink:"/hackathon-book-enhanced/docs/modules/week-13-conversational/multi-modal-interaction",draft:!1,unlisted:!1,editUrl:"https://github.com/owaismukhtarkhan/hackathon-book/tree/main/docs/modules/week-13-conversational/multi-modal-interaction.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Multi-Modal Interaction in Robotics",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Speech Recognition for Robotics",permalink:"/hackathon-book-enhanced/docs/modules/week-13-conversational/speech-recognition"},next:{title:"ROS 2 Assessment",permalink:"/hackathon-book-enhanced/docs/modules/week-03-05-ros2/ros2-assessment"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Multi-Modal Interaction Concepts",id:"multi-modal-interaction-concepts",level:2},{value:"What is Multi-Modal Interaction?",id:"what-is-multi-modal-interaction",level:3},{value:"Vision-Language-Action (VLA) Framework",id:"vision-language-action-vla-framework",level:3},{value:"Implementing Multi-Modal Systems",id:"implementing-multi-modal-systems",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Multi-Modal Data Fusion",id:"multi-modal-data-fusion",level:3},{value:"Vision-Language Integration with Google Gemini",id:"vision-language-integration-with-google-gemini",level:2},{value:"Processing Images with Language Commands",id:"processing-images-with-language-commands",level:3},{value:"Advanced Multi-Modal Techniques",id:"advanced-multi-modal-techniques",level:2},{value:"Attention Mechanisms for Multi-Modal Data",id:"attention-mechanisms-for-multi-modal-data",level:3},{value:"Conversational Robotics with Visual Context",id:"conversational-robotics-with-visual-context",level:2},{value:"Maintaining Conversation State with Visual Memory",id:"maintaining-conversation-state-with-visual-memory",level:3},{value:"Practical Exercise: Complete Conversational Robot",id:"practical-exercise-complete-conversational-robot",level:2},{value:"Objective",id:"objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Example Multi-Modal Commands",id:"example-multi-modal-commands",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"multi-modal-interaction-in-robotics",children:"Multi-Modal Interaction in Robotics"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Multi-modal interaction combines multiple sensory inputs (vision, speech, touch) and outputs to create more natural and robust human-robot interfaces. This module focuses on Vision-Language-Action (VLA) pipelines that integrate visual perception, natural language understanding, and robotic action execution using Google Gemini as required by our constitution."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design multi-modal interaction systems combining vision and language"}),"\n",(0,t.jsx)(n.li,{children:"Implement Vision-Language-Action (VLA) pipelines using Google Gemini"}),"\n",(0,t.jsx)(n.li,{children:"Create robust interfaces that handle multiple input modalities"}),"\n",(0,t.jsx)(n.li,{children:"Build conversational robotics systems with visual context awareness"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed language models in robotics module"}),"\n",(0,t.jsx)(n.li,{children:"Completed speech recognition module"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of sensor integration in robotics"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"multi-modal-interaction-concepts",children:"Multi-Modal Interaction Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-multi-modal-interaction",children:"What is Multi-Modal Interaction?"}),"\n",(0,t.jsx)(n.p,{children:"Multi-modal interaction involves:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple input modalities"}),": Vision, speech, gesture, touch"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple output modalities"}),": Speech, movement, visual feedback"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context integration"}),": Combining information from different sources"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural interaction"}),": More human-like communication patterns"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"vision-language-action-vla-framework",children:"Vision-Language-Action (VLA) Framework"}),"\n",(0,t.jsx)(n.p,{children:"The VLA framework connects:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": Processing visual input from cameras and sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Understanding and generating natural language"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Executing robot behaviors and motor commands"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementing-multi-modal-systems",children:"Implementing Multi-Modal Systems"}),"\n",(0,t.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Camera Input] --\x3e B[Vision Processing]\n    C[Microphone Input] --\x3e D[Speech Recognition]\n    B --\x3e E[Google Gemini Integration]\n    D --\x3e E\n    E --\x3e F[Action Planning]\n    F --\x3e G[Robot Execution]\n    G --\x3e H[Feedback Loop]\n    H --\x3e B\n"})}),"\n",(0,t.jsx)(n.h3,{id:"multi-modal-data-fusion",children:"Multi-Modal Data Fusion"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# multi_modal_fusion.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CompressedImage\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PointStamped\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport google.generativeai as genai\nimport json\nimport base64\nfrom io import BytesIO\n\nclass MultiModalFusion(Node):\n    def __init__(self):\n        super().__init__(\'multi_modal_fusion\')\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.model = genai.GenerativeModel(\'gemini-pro-vision\')\n\n        # Latest sensor data storage\n        self.latest_image = None\n        self.latest_command = None\n        self.latest_objects = []\n\n        # Subscribers\n        self.image_subscription = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.command_subscription = self.create_subscription(\n            String,\n            \'recognized_speech\',\n            self.command_callback,\n            10\n        )\n\n        self.objects_subscription = self.create_subscription(\n            String,\n            \'detected_objects\',\n            self.objects_callback,\n            10\n        )\n\n        # Publisher for fused understanding\n        self.fused_publisher = self.create_publisher(\n            String,\n            \'multi_modal_understanding\',\n            10\n        )\n\n        # Publisher for robot actions\n        self.action_publisher = self.create_publisher(\n            String,\n            \'robot_actions\',\n            10\n        )\n\n        # Timer for processing fused data\n        self.process_timer = self.create_timer(1.0, self.process_fused_data)\n\n        self.get_logger().info(\'Multi-modal fusion node initialized\')\n\n    def image_callback(self, msg):\n        """Handle incoming image data"""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.latest_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f\'Error converting image: {e}\')\n\n    def command_callback(self, msg):\n        """Handle incoming command data"""\n        self.latest_command = msg.data\n\n    def objects_callback(self, msg):\n        """Handle incoming object detection data"""\n        try:\n            self.latest_objects = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error(\'Error parsing object detection data\')\n\n    def process_fused_data(self):\n        """Process multi-modal data when all inputs are available"""\n        if self.latest_image is not None and self.latest_command is not None:\n            # Process with Google Gemini\n            understanding = self.process_with_gemini_vision(\n                self.latest_image,\n                self.latest_command,\n                self.latest_objects\n            )\n\n            if understanding:\n                # Publish fused understanding\n                fused_msg = String()\n                fused_msg.data = json.dumps(understanding)\n                self.fused_publisher.publish(fused_msg)\n\n                # Publish actions if available\n                if \'action_sequence\' in understanding:\n                    action_msg = String()\n                    action_msg.data = json.dumps(understanding[\'action_sequence\'])\n                    self.action_publisher.publish(action_msg)\n\n                self.get_logger().info(f\'Processed multi-modal command: {self.latest_command}\')\n\n            # Clear processed data\n            self.latest_command = None\n\n    def process_with_gemini_vision(self, image, command, objects):\n        """Process image and text with Gemini Pro Vision"""\n        try:\n            # Convert OpenCV image to base64 for Gemini\n            _, buffer = cv2.imencode(\'.jpg\', image)\n            image_bytes = buffer.tobytes()\n\n            # Prepare prompt\n            prompt = f"""\n            You are a robot perception and action planning system. Based on the image provided,\n            understand the scene and interpret the following command: "{command}"\n\n            Detected objects in the scene: {json.dumps(objects)}\n\n            Provide a JSON response with:\n            1. scene_understanding: Description of what you see in the image\n            2. command_interpretation: How the command should be interpreted in this context\n            3. action_sequence: Specific actions to execute\n            4. target_object: Object related to the command (if applicable)\n            5. spatial_relationships: Relationships between objects relevant to the command\n            6. confidence_score: Confidence in interpretation (0.0 to 1.0)\n            """\n\n            # Create PIL image from bytes for Gemini\n            import PIL.Image\n            pil_image = PIL.Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n            # Call Gemini vision model\n            response = self.model.generate_content([prompt, pil_image])\n\n            # Parse response\n            response_text = response.text.strip().strip(\'```json\').strip(\'`\')\n            result = json.loads(response_text)\n            return result\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing with Gemini Vision: {e}\')\n            return {\n                "scene_understanding": "Failed to process image",\n                "command_interpretation": "Failed to interpret command",\n                "action_sequence": ["error"],\n                "target_object": None,\n                "spatial_relationships": [],\n                "confidence_score": 0.0\n            }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = MultiModalFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"vision-language-integration-with-google-gemini",children:"Vision-Language Integration with Google Gemini"}),"\n",(0,t.jsx)(n.h3,{id:"processing-images-with-language-commands",children:"Processing Images with Language Commands"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def process_visual_language_command(self, image, command, context):\n    """Process a visual scene with a language command using Gemini"""\n\n    # Prepare the multi-modal prompt\n    prompt = f"""\n    You are a robot assistant with vision capabilities. You see the following scene:\n\n    The user wants you to: "{command}"\n\n    Additional context: {context}\n\n    Analyze the image and provide specific actions. Respond with:\n    1. object_location: Where the relevant object is in the image (x, y coordinates)\n    2. action_plan: Sequence of actions to accomplish the goal\n    3. safety_considerations: Any safety issues to consider\n    4. alternative_interpretations: Other possible interpretations of the command\n    """\n\n    # Use Gemini Pro Vision for multi-modal processing\n    model = genai.GenerativeModel(\'gemini-pro-vision\')\n\n    # Process the image and text together\n    response = model.generate_content([\n        prompt,\n        image  # PIL Image object\n    ])\n\n    return self.parse_gemini_response(response.text)\n\ndef parse_gemini_response(self, response_text):\n    """Parse the response from Gemini into structured data"""\n    try:\n        # Remove code block markers if present\n        clean_response = response_text.strip().strip(\'```json\').strip(\'`\')\n        result = json.loads(clean_response)\n        return result\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to extract the key information\n        return {\n            "object_location": None,\n            "action_plan": ["unknown"],\n            "safety_considerations": [],\n            "alternative_interpretations": [response_text]\n        }\n'})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-multi-modal-techniques",children:"Advanced Multi-Modal Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"attention-mechanisms-for-multi-modal-data",children:"Attention Mechanisms for Multi-Modal Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultiModalAttention:\n    def __init__(self):\n        # In a real implementation, this would contain neural network models\n        pass\n\n    def fuse_modalities(self, visual_features, linguistic_features):\n        """Fuse visual and linguistic features using attention mechanisms"""\n        # This is a conceptual implementation\n        # In practice, you\'d use deep learning models\n        fused_features = {\n            \'visual_context\': visual_features,\n            \'linguistic_context\': linguistic_features,\n            \'attention_weights\': self.compute_attention_weights(visual_features, linguistic_features)\n        }\n        return fused_features\n\n    def compute_attention_weights(self, visual, linguistic):\n        """Compute attention weights between modalities"""\n        # Placeholder implementation\n        return {"visual_weight": 0.6, "linguistic_weight": 0.4}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"conversational-robotics-with-visual-context",children:"Conversational Robotics with Visual Context"}),"\n",(0,t.jsx)(n.h3,{id:"maintaining-conversation-state-with-visual-memory",children:"Maintaining Conversation State with Visual Memory"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ConversationalRobot:\n    def __init__(self):\n        self.model = genai.GenerativeModel('gemini-pro')\n        self.conversation_history = []\n        self.visual_memory = {}  # Store objects seen in different contexts\n        self.current_scene = None\n\n    def update_visual_memory(self, image, detected_objects):\n        \"\"\"Update visual memory with current scene\"\"\"\n        import hashlib\n\n        # Create a hash of the current scene for reference\n        scene_hash = hashlib.md5(str(detected_objects).encode()).hexdigest()\n\n        self.visual_memory[scene_hash] = {\n            'timestamp': self.get_current_time(),\n            'objects': detected_objects,\n            'image_reference': scene_hash,\n            'associated_commands': []\n        }\n\n        self.current_scene = scene_hash\n\n    def process_conversational_command(self, command, image):\n        \"\"\"Process a command in conversational context with visual information\"\"\"\n\n        # Prepare context including conversation history and visual information\n        context = {\n            'conversation_history': self.conversation_history[-5:],  # Last 5 exchanges\n            'current_objects': self.visual_memory.get(self.current_scene, {}).get('objects', []),\n            'current_scene_hash': self.current_scene\n        }\n\n        prompt = f\"\"\"\n        You are a conversational robot with visual memory. You're in a conversation with a user.\n\n        Conversation History:\n        {json.dumps(context['conversation_history'])}\n\n        Current Scene Objects:\n        {json.dumps(context['current_objects'])}\n\n        User says: \"{command}\"\n\n        Respond appropriately considering both the conversation context and visual information.\n        Provide actions to execute and update the conversation state.\n\n        Respond with JSON containing:\n        1. response_text: What the robot should say back\n        2. action_sequence: Actions to execute\n        3. update_memory: Whether to update visual memory\n        4. conversation_continuation: Whether this continues the conversation\n        \"\"\"\n\n        response = self.model.generate_content(prompt)\n        parsed_response = self.parse_gemini_response(response.text)\n\n        # Update conversation history\n        self.conversation_history.append({\n            'user': command,\n            'robot': parsed_response.get('response_text', ''),\n            'timestamp': self.get_current_time()\n        })\n\n        # Limit conversation history to prevent memory issues\n        if len(self.conversation_history) > 20:\n            self.conversation_history = self.conversation_history[-10:]\n\n        return parsed_response\n"})}),"\n",(0,t.jsx)(n.h2,{id:"practical-exercise-complete-conversational-robot",children:"Practical Exercise: Complete Conversational Robot"}),"\n",(0,t.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Create a complete multi-modal conversational robot system that responds to voice commands with visual context awareness."}),"\n",(0,t.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up multi-modal fusion node"}),"\n",(0,t.jsx)(n.li,{children:"Implement visual context understanding"}),"\n",(0,t.jsx)(n.li,{children:"Create conversational state management"}),"\n",(0,t.jsx)(n.li,{children:"Test with complex multi-modal commands"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-multi-modal-commands",children:"Example Multi-Modal Commands"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Pick up the red cup you see in front of you"'}),"\n",(0,t.jsx)(n.li,{children:'"Navigate to the room with the blue chair"'}),"\n",(0,t.jsx)(n.li,{children:'"Show me the object I was pointing at before"'}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,t.jsx)(n.p,{children:"Students must demonstrate the following:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Successfully implement multi-modal data fusion"}),"\n",(0,t.jsx)(n.li,{children:"Integrate vision and language using Google Gemini"}),"\n",(0,t.jsx)(n.li,{children:"Create a conversational robot with visual context awareness"}),"\n",(0,t.jsx)(n.li,{children:"Handle complex multi-modal commands effectively"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Success Threshold: 80% success rate in multi-modal command processing"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:'After completing this module, students will have a complete understanding of Vision-Language-Action pipelines and will be ready to implement the capstone project "The Autonomous Humanoid" with all required capabilities.'})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);