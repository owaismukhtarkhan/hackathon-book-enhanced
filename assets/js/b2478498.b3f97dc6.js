"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[219],{3114(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>d});var a=i(4848),s=i(8453);const o={title:"Sensor Simulation in Robotics",sidebar_position:3},r="Sensor Simulation in Robotics",t={id:"modules/week-06-07-simulation/sensor-simulation",title:"Sensor Simulation in Robotics",description:"Overview",source:"@site/docs/modules/week-06-07-simulation/sensor-simulation.md",sourceDirName:"modules/week-06-07-simulation",slug:"/modules/week-06-07-simulation/sensor-simulation",permalink:"/hackathon-book-enhanced/docs/modules/week-06-07-simulation/sensor-simulation",draft:!1,unlisted:!1,editUrl:"https://github.com/owaismukhtarkhan/hackathon-book/tree/main/docs/modules/week-06-07-simulation/sensor-simulation.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Sensor Simulation in Robotics",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Unity Visualization for Robotics",permalink:"/hackathon-book-enhanced/docs/modules/week-06-07-simulation/unity-visualization"},next:{title:"Language Models in Robotics",permalink:"/hackathon-book-enhanced/docs/modules/week-13-conversational/language-models-robotics"}},l={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"LiDAR Sensor Simulation",id:"lidar-sensor-simulation",level:2},{value:"Gazebo LiDAR Configuration",id:"gazebo-lidar-configuration",level:3},{value:"Unity LiDAR Simulation",id:"unity-lidar-simulation",level:3},{value:"Camera and Depth Sensor Simulation",id:"camera-and-depth-sensor-simulation",level:2},{value:"RGB Camera Configuration",id:"rgb-camera-configuration",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"IMU Sensor Simulation",id:"imu-sensor-simulation",level:2},{value:"IMU Configuration in Gazebo",id:"imu-configuration-in-gazebo",level:3},{value:"Sensor Data Validation",id:"sensor-data-validation",level:2},{value:"Validation Framework",id:"validation-framework",level:3},{value:"Practical Exercise: Multi-Sensor Robot Simulation",id:"practical-exercise-multi-sensor-robot-simulation",level:2},{value:"Objective",id:"objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Example Robot with Multiple Sensors",id:"example-robot-with-multiple-sensors",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"sensor-simulation-in-robotics",children:"Sensor Simulation in Robotics"}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is critical for developing and testing robotic systems in virtual environments. This module covers simulating various types of sensors including LiDAR, cameras, IMUs, and other perception sensors that provide the data needed for robot navigation, mapping, and decision-making."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Configure and validate LiDAR sensor simulation in Gazebo and Unity"}),"\n",(0,a.jsx)(e.li,{children:"Set up camera and depth sensor simulation"}),"\n",(0,a.jsx)(e.li,{children:"Implement IMU and other inertial sensor simulation"}),"\n",(0,a.jsx)(e.li,{children:"Validate sensor data streams for accuracy and reliability"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Completed Gazebo setup and Unity visualization modules"}),"\n",(0,a.jsx)(e.li,{children:"Understanding of basic sensor types and their applications"}),"\n",(0,a.jsx)(e.li,{children:"Basic knowledge of ROS 2 sensor message types"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"lidar-sensor-simulation",children:"LiDAR Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"gazebo-lidar-configuration",children:"Gazebo LiDAR Configuration"}),"\n",(0,a.jsxs)(e.p,{children:["LiDAR sensors in Gazebo are configured using the ",(0,a.jsx)(e.code,{children:"<sensor>"})," tag in SDF files:"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar" type="ray">\n  <pose>0 0 0.2 0 0 0</pose>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>360</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <namespace>/lidar</namespace>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"unity-lidar-simulation",children:"Unity LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Using Unity's Perception package for LiDAR simulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing Unity.Perception.GroundTruth;\n\npublic class LiDARSetup : MonoBehaviour\n{\n    public int rayCount = 360;\n    public float range = 30.0f;\n    public float fieldOfView = 360.0f;\n\n    void Start()\n    {\n        // Configure LiDAR sensor using Perception package\n        var lidarSensor = GetComponent<LiDARSensor>();\n        lidarSensor.rayCount = rayCount;\n        lidarSensor.range = range;\n        lidarSensor.fieldOfView = fieldOfView;\n    }\n}\n"})}),"\n",(0,a.jsx)(e.h2,{id:"camera-and-depth-sensor-simulation",children:"Camera and Depth Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"rgb-camera-configuration",children:"RGB Camera Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Gazebo camera sensor --\x3e\n<sensor name="camera" type="camera">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>100</far>\n    </clip>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>/camera</namespace>\n      <remapping>~/image_raw:=image_raw</remapping>\n      <remapping>~/camera_info:=camera_info</remapping>\n    </ros>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Gazebo depth camera --\x3e\n<sensor name="depth_camera" type="depth">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n  <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n    <ros>\n      <namespace>/depth_camera</namespace>\n      <remapping>~/image_raw:=image_raw</remapping>\n      <remapping>~/depth/image_raw:=depth/image_raw</remapping>\n    </ros>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"imu-sensor-simulation",children:"IMU Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"imu-configuration-in-gazebo",children:"IMU Configuration in Gazebo"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>1</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n    <ros>\n      <namespace>/imu</namespace>\n      <remapping>~/out:=data</remapping>\n    </ros>\n    <initial_orientation_as_reference>false</initial_orientation_as_reference>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-data-validation",children:"Sensor Data Validation"}),"\n",(0,a.jsx)(e.h3,{id:"validation-framework",children:"Validation Framework"}),"\n",(0,a.jsx)(e.p,{children:"Create a validation framework to ensure sensor data is accurate and reliable:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# sensor_validator.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nimport numpy as np\n\nclass SensorValidator(Node):\n    def __init__(self):\n        super().__init__('sensor_validator')\n\n        # Subscribe to sensor topics\n        self.lidar_subscription = self.create_subscription(\n            LaserScan,\n            '/lidar/scan',\n            self.lidar_callback,\n            10)\n\n        self.camera_subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.camera_callback,\n            10)\n\n        self.imu_subscription = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10)\n\n        self.get_logger().info('Sensor validator initialized')\n\n    def lidar_callback(self, msg):\n        # Validate LiDAR data\n        ranges = np.array(msg.ranges)\n\n        # Check for valid range values\n        valid_ranges = ranges[(ranges >= msg.range_min) & (ranges <= msg.range_max)]\n\n        # Calculate validation metrics\n        if len(valid_ranges) / len(ranges) < 0.95:\n            self.get_logger().warn('LiDAR data has too many invalid ranges')\n        else:\n            self.get_logger().info('LiDAR data validation passed')\n\n    def camera_callback(self, msg):\n        # Validate camera data\n        height = msg.height\n        width = msg.width\n        data_size = len(msg.data)\n\n        # Check image dimensions and data consistency\n        expected_size = height * width * 3  # Assuming RGB\n        if data_size != expected_size:\n            self.get_logger().warn('Camera data size mismatch')\n        else:\n            self.get_logger().info('Camera data validation passed')\n\n    def imu_callback(self, msg):\n        # Validate IMU data\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        linear_acceleration = msg.linear_acceleration\n\n        # Check for valid quaternion (normalized)\n        norm = np.sqrt(orientation.x**2 + orientation.y**2 +\n                      orientation.z**2 + orientation.w**2)\n\n        if abs(norm - 1.0) > 0.01:\n            self.get_logger().warn('IMU orientation quaternion not normalized')\n        else:\n            self.get_logger().info('IMU data validation passed')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SensorValidator()\n    rclpy.spin(validator)\n    validator.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"practical-exercise-multi-sensor-robot-simulation",children:"Practical Exercise: Multi-Sensor Robot Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"objective",children:"Objective"}),"\n",(0,a.jsx)(e.p,{children:"Create a robot model with multiple sensors (LiDAR, camera, IMU) and validate all sensor data streams."}),"\n",(0,a.jsx)(e.h3,{id:"steps",children:"Steps"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Create a robot model with LiDAR, camera, and IMU sensors"}),"\n",(0,a.jsx)(e.li,{children:"Configure each sensor with appropriate parameters"}),"\n",(0,a.jsx)(e.li,{children:"Launch the simulation and verify sensor topics"}),"\n",(0,a.jsx)(e.li,{children:"Run validation checks on all sensor data streams"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"example-robot-with-multiple-sensors",children:"Example Robot with Multiple Sensors"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="multi_sensor_robot">\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.2" length="0.1"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- LiDAR sensor --\x3e\n  <joint name="lidar_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0 0 0.3" rpy="0 0 0"/>\n  </joint>\n\n  <link name="lidar_link">\n    \x3c!-- LiDAR sensor definition --\x3e\n    <sensor name="lidar" type="ray">\n      <pose>0 0 0 0 0 0</pose>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>360</samples>\n            <resolution>1</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n    </sensor>\n  </link>\n\n  \x3c!-- Camera sensor --\x3e\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.1 0 0.2" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    \x3c!-- Camera sensor definition --\x3e\n    <sensor name="camera" type="camera">\n      <pose>0 0 0 0 0 0</pose>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>100</far>\n        </clip>\n      </camera>\n    </sensor>\n  </link>\n\n  \x3c!-- IMU sensor --\x3e\n  <joint name="imu_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n    <origin xyz="0 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="imu_link">\n    \x3c!-- IMU sensor definition --\x3e\n    <sensor name="imu" type="imu">\n      <pose>0 0 0 0 0 0</pose>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.001</stddev>\n            </noise>\n          </x>\n        </angular_velocity>\n      </imu>\n    </sensor>\n  </link>\n</robot>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"assessment",children:"Assessment"}),"\n",(0,a.jsx)(e.p,{children:"Students must demonstrate the following:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Successfully configure multiple sensor types on a robot model"}),"\n",(0,a.jsx)(e.li,{children:"Validate sensor data streams for accuracy and reliability"}),"\n",(0,a.jsx)(e.li,{children:"Implement sensor fusion concepts in simulation"}),"\n",(0,a.jsx)(e.li,{children:"Achieve 85% accuracy threshold in sensor validation tests"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Success Threshold: 85% accuracy in sensor simulation validation"})}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"After completing this module, students should have a solid understanding of simulation environments and sensor modeling, preparing them for more advanced topics in AI integration with robotics systems."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>t});var a=i(6540);const s={},o=a.createContext(s);function r(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);